#!/usr/bin/env python3
"""
æ•°æ®æºçŠ¶æ€æ£€æŸ¥å·¥å…·
æ£€æŸ¥RSSæºçš„å¯ç”¨æ€§å’Œå†…å®¹æ–°é²œåº¦
"""

import feedparser
import requests
from datetime import datetime, timedelta
import json

def check_rss_source(url, name):
    """æ£€æŸ¥å•ä¸ªRSSæºçš„çŠ¶æ€"""
    print(f"\nğŸ“¡ æ£€æŸ¥ {name}")
    print(f"ğŸ”— URL: {url}")
    
    try:
        # è®¾ç½®è¶…æ—¶å’Œç”¨æˆ·ä»£ç†
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # å…ˆæ£€æŸ¥HTTPçŠ¶æ€
        response = requests.get(url, headers=headers, timeout=10)
        print(f"ğŸ“Š HTTPçŠ¶æ€: {response.status_code}")
        
        if response.status_code != 200:
            print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
            return False
        
        # è§£æRSS
        feed = feedparser.parse(url)
        
        if not hasattr(feed, 'entries') or len(feed.entries) == 0:
            print("âŒ æ— æ³•è·å–RSSå†…å®¹æˆ–å†…å®¹ä¸ºç©º")
            return False
        
        print(f"âœ… æˆåŠŸè·å– {len(feed.entries)} ç¯‡æ–‡ç« ")
        
        # æ£€æŸ¥æœ€æ–°æ–‡ç« æ—¶é—´
        if hasattr(feed.entries[0], 'published_parsed'):
            pub_time = datetime(*feed.entries[0].published_parsed[:6])
            days_ago = (datetime.now() - pub_time).days
            print(f"ğŸ“… æœ€æ–°æ–‡ç« : {feed.entries[0].title[:50]}...")
            print(f"ğŸ•’ å‘å¸ƒæ—¶é—´: {days_ago} å¤©å‰")
            
            if days_ago > 7:
                print("âš ï¸ å†…å®¹å¯èƒ½ä¸å¤Ÿæ–°é²œ")
        
        # æ£€æŸ¥AIç›¸å…³å†…å®¹
        ai_keywords = [
            'ai', 'artificial intelligence', 'chatgpt', 'gpt', 'claude',
            'midjourney', 'dall-e', 'machine learning', 'ai tool'
        ]
        
        ai_count = 0
        for entry in feed.entries[:10]:
            content = f"{entry.title} {getattr(entry, 'summary', '')}".lower()
            if any(keyword in content for keyword in ai_keywords):
                ai_count += 1
        
        print(f"ğŸ¤– AIç›¸å…³å†…å®¹: {ai_count}/10 ç¯‡")
        
        if ai_count < 3:
            print("âš ï¸ AIç›¸å…³å†…å®¹è¾ƒå°‘")
        
        return True
        
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶")
        return False
    except requests.exceptions.ConnectionError:
        print("âŒ è¿æ¥é”™è¯¯")
        return False
    except Exception as e:
        print(f"âŒ å…¶ä»–é”™è¯¯: {e}")
        return False

def check_all_sources():
    """æ£€æŸ¥æ‰€æœ‰æ•°æ®æº"""
    print("ğŸ” AIå·¥å…·èµ„è®¯æ•°æ®æºçŠ¶æ€æ£€æŸ¥")
    print("=" * 50)
    
    # å½“å‰ä½¿ç”¨çš„RSSæº
    sources = [
        ("TechCrunch AI", "https://techcrunch.com/category/artificial-intelligence/feed/"),
        ("The Verge AI", "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml"),
        ("VentureBeat AI", "https://venturebeat.com/ai/feed/"),
        ("Machine Learning Mastery", "https://machinelearningmastery.com/feed/"),
    ]
    
    # å¤‡é€‰RSSæº
    alternative_sources = [
        ("AI News", "https://artificialintelligence-news.com/feed/"),
        ("MIT Technology Review AI", "https://www.technologyreview.com/topic/artificial-intelligence/feed/"),
        ("Towards Data Science", "https://towardsdatascience.com/feed"),
        ("AI Research", "https://www.airesearch.com/feed/"),
    ]
    
    working_sources = []
    
    print("\nğŸ“Š å½“å‰æ•°æ®æºæ£€æŸ¥:")
    for name, url in sources:
        if check_rss_source(url, name):
            working_sources.append((name, url))
    
    print(f"\nâœ… å¯ç”¨æ•°æ®æº: {len(working_sources)}/{len(sources)}")
    
    if len(working_sources) < 2:
        print("\nğŸ”„ æ£€æŸ¥å¤‡é€‰æ•°æ®æº:")
        for name, url in alternative_sources:
            if check_rss_source(url, name):
                working_sources.append((name, url))
                if len(working_sources) >= 3:
                    break
    
    print("\n" + "=" * 50)
    print("ğŸ“‹ æ¨èä½¿ç”¨çš„æ•°æ®æº:")
    for i, (name, url) in enumerate(working_sources[:3], 1):
        print(f"{i}. {name}")
        print(f"   {url}")
    
    if len(working_sources) == 0:
        print("\nâš ï¸ æ‰€æœ‰RSSæºéƒ½æ— æ³•è®¿é—®")
        print("ğŸ’¡ å»ºè®®:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®")
        print("2. ä½¿ç”¨ç²¾é€‰å·¥å…·åº“æ¨¡å¼")
        print("3. è€ƒè™‘æ‰‹åŠ¨æ›´æ–°å·¥å…·ä¿¡æ¯")
    
    return working_sources

def suggest_improvements():
    """æä¾›æ”¹è¿›å»ºè®®"""
    print("\nğŸ’¡ èµ„è®¯æ¥æºæ”¹è¿›å»ºè®®:")
    print("-" * 30)
    
    suggestions = [
        "1. ğŸ“š ä½¿ç”¨ç²¾é€‰å·¥å…·åº“ (æ¨è)",
        "   - æ‰‹åŠ¨ç­›é€‰é«˜è´¨é‡AIå·¥å…·",
        "   - å®šæœŸæ›´æ–°(å»ºè®®æ¯æœˆä¸€æ¬¡)",
        "   - ç¡®ä¿ä¿¡æ¯å‡†ç¡®æ€§",
        "",
        "2. ğŸ”„ å¤šæºç»“åˆç­–ç•¥",
        "   - RSSæº + ç²¾é€‰åº“ + æ‰‹åŠ¨æ›´æ–°",
        "   - é™ä½å¯¹å•ä¸€æ•°æ®æºçš„ä¾èµ–",
        "   - æé«˜å†…å®¹å¤šæ ·æ€§",
        "",
        "3. ğŸ“± ç¤¾äº¤åª’ä½“ç›‘æ§",
        "   - å…³æ³¨AIå·¥å…·ç›¸å…³Twitterè´¦å·",
        "   - ç›‘æ§Product Huntæ–°äº§å“",
        "   - å…³æ³¨GitHub Trending",
        "",
        "4. ğŸ¤– AIè¾…åŠ©å†…å®¹ç”Ÿæˆ",
        "   - è®©AIæ ¹æ®å½“å‰çƒ­ç‚¹ç”Ÿæˆå†…å®¹",
        "   - ç»“åˆæ—¶äº‹å’Œè¶‹åŠ¿",
        "   - ä¿æŒå†…å®¹æ–°é²œåº¦"
    ]
    
    for suggestion in suggestions:
        print(suggestion)

if __name__ == "__main__":
    working_sources = check_all_sources()
    suggest_improvements()
    
    print(f"\nğŸ“Š æ£€æŸ¥å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
